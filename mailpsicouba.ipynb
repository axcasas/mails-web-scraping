{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your First Web Scrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see how to make a simple python code for web scrapping. This will give you a basic intuition of how all the process work. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once upon a time..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So i was doing my Bachelor's thesis and about the relation between chronotypes, digital device use, and light exposure. But I also wanted to study how students sleep!\n",
    "\n",
    "So I needed a big number of responses in order to get statistical significance. Thus I decided to send an email to ALL the subjects of my degree. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a website with all the email addresses, but I dind't want to copy-paste all of them. That would take me a lot of time!\n",
    "\n",
    "So what if I just Python to create a csv file of all those emails from that website?\n",
    "\n",
    "Hopefully, this notebook will get you the intuition about web scrapping with Python. \n",
    "\n",
    "Let's go into it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd # for the csv file\n",
    "import requests # to get the info from a website\n",
    "import re # to scrap and find all the emails in the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first set a variable \n",
    "emails = set() # this is to avoid duplicates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function to get the html info of that website. You can simply do this by using the requests.get() function. To debug, it may be useful to make a small function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get HTML content \n",
    "def get_html(url):\n",
    "    try: \n",
    "        response = requests.get(url)\n",
    "        return response.text\n",
    "    except: # we use try to get an error if something goes bad\n",
    "        return \"Failed\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now we can get all the data. But secondly we need to find all the emails from that html file!\n",
    "\n",
    "How? Using the re library and some regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract emails from HTML content\n",
    "def get_emails(html):\n",
    "    new_emails = re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", html) # this weird letters are needed to find the specific pattern of the email adresses\n",
    "    emails.update(new_emails)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Now we can start doing web scrapping! Let's set a variable for the url and for the get_html() function we did before. \n",
    "\n",
    "We can also use the get_emails() function to extract emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://edipsicouba.net.ar/info-util/4136/\n"
     ]
    }
   ],
   "source": [
    "# Scrapping \n",
    "url = \"https://edipsicouba.net.ar/info-util/4136/\" # link for the website you wanna scrap\n",
    "print(f\"Scraping {url}\") # Print progress\n",
    "html = get_html(url) # Get HTML content\n",
    "get_emails(html) # Extract emails"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! If you wanna see if everying went well, you can print get_html() or get_emails(). Remember the try conditional? If something went bad it will say \"Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_html(url)) # print get_html() to see if there's the html file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we will use pandas module to create a csv file for all the email addresses that we found.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a dataframe from the emails \n",
    "df = pd.DataFrame(emails, columns=['emails'])\n",
    "df.shape # to check the number of the emails we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 46 emails to psico_mails.csv\n"
     ]
    }
   ],
   "source": [
    "# export dataframe to a csv file \n",
    "df.to_csv('psico_mails.csv', index=False)\n",
    "print(f'Saved {len(emails)} emails to psico_mails.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You web scrapped an url and got specific info (email addresses) for your convenience. \n",
    "\n",
    "Now you should have an intuition of how web scrapping works and automate some boring stuff in python (yes, there's a great book called like that). \n",
    "\n",
    "Now go and make this code more complex for your specific objectives!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
